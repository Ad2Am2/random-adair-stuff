{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ad2Am2/random-adair-stuff/blob/main/LLM_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q vllm faiss-cpu accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "I0gbwp2Zg2QK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57452ab9-d65f-4a3c-fd91-83b1756cdc02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.4/94.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "yfinance 0.2.59 requires protobuf<6,>=5.29.0, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "# Assumes current path\n",
        "CPDLC_DOWNLINK_PATH = \"CPDLC_Downlink.json\"\n",
        "CPDLC_UPLINK_PATH = \"CPDLC_Uplink.json\"\n",
        "\n",
        "# 'Abstract' Class that provides confined constants for different purposes\n",
        "class User:\n",
        "    def __init__(self):\n",
        "        self.cpdlc_data = []\n",
        "\n",
        "    def get_cpdlc_data(self) -> list:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_system_prompt(self) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Loads CPDLC data from parent class\n",
        "    def _load_data(self, path):\n",
        "        try:\n",
        "            with open(path, \"r\") as cpdlc_json:\n",
        "                self.cpdlc_data = json.load(cpdlc_json)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error occured when reading the CPDLC json data from path '{path}'. Error Description:\\n\")\n",
        "            print(sys.exc_info()[0])\n",
        "\n",
        "# When a Pilot uses the application\n",
        "class Pilot(User):\n",
        "    def __init__(self, cpdlc_data_path=None) -> None:\n",
        "        super().__init__()\n",
        "        self.path = CPDLC_DOWNLINK_PATH if cpdlc_data_path is None else cpdlc_data_path\n",
        "        self._load_data(self.path)\n",
        "\n",
        "    def get_cpdlc_data(self) -> list:\n",
        "        return self.cpdlc_data\n",
        "\n",
        "    def get_system_prompt(self) -> str:\n",
        "        return \"\"\"You are a CPDLC translation expert. More specifically, you are tasked with extracting ONE CPDLC instruction/message from a natural language based message/instruction from a pilot to an Air Traffic Controller (ATC).\n",
        "        As general guidelines: The message is in the context of aviation, your outputs should reflect that. For example, flight level X for some number X should follow the convention of FLX, etc.\n",
        "        If you include a quantity which is associated with an explicit unit in your output, the unit should always be after the quantity.\n",
        "        Do not add unnecessary complexity to CPDLC messages if it is not stated. For example, do not choose a cpdlc message containing when able unless it is specified or implied to do the instruction when able.\n",
        "        Do not change the units string contained in the input message unless you are confident that the abreviation you use is accurate. For example, do not change KNOTS to KTS.\n",
        "        If you output a time, it should follow the 24-hour clock (hh:mm) format.\n",
        "\n",
        "        Use these relevant CPDLC messages descriptors (each descriptor contains respective fields enclosed by single quotes for CPDLC Message where words surrounded with [] imply an INPUT that MUST be filled in by you if the message is chosen as translation, Intent and Reference Number) as context and choose the appropriate translation based on the intent of the input message:\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Translate this pilot/ATC message to CPDLC format and respond ONLY in JSON format where the response should ONLY contain ONE CPDLC translation as follows:\n",
        "        {{\n",
        "        \"reference\": [The correct reference number associated with the corresponding CPDLC Message translation],\n",
        "        \"message\": [Translated CPDLC Message],\n",
        "        \"context\": [Additional information that the pilot cannot convey through the CPDLC Message that should be given to the ATC. This field should be empty if no additional relevant information is present in the input based on intent.]\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "# When an ATC uses the application\n",
        "class ATC(User):\n",
        "    def __init__(self, cpdlc_data_path=None) -> None:\n",
        "        super().__init__()\n",
        "        self.path = CPDLC_UPLINK_PATH if cpdlc_data_path is None else cpdlc_data_path\n",
        "        self._load_data(self.path)\n",
        "\n",
        "    def get_cpdlc_data(self) -> list:\n",
        "        return self.cpdlc_data\n",
        "\n",
        "    def get_system_prompt(self) -> str:\n",
        "        return \"\"\"You are a CPDLC translation expert. More specifically, you are tasked with extracting ONE CPDLC instruction/message from a natural language based message/instruction from an Air Traffic Controller (ATC) to a pilot.\n",
        "        As general guidelines: The message is in the context of aviation, your outputs should reflect that. For example, flight level X for some number X should follow the convention of FLX, etc.\n",
        "        If you include a quantity which is associated with an explicit unit in your output, the unit should always be after the quantity.\n",
        "        Do not add unnecessary complexity to CPDLC messages if it is not stated. For example, do not choose a cpdlc message containing when able unless it is specified or implied to do the instruction when able.\n",
        "        Do not change the units string contained in the input message unless you are confident that the abreviation you use is accurate. For example, do not change KNOTS to KTS.\n",
        "        If you output a time, it should follow the 24-hour clock (hh:mm) format.\n",
        "\n",
        "        Use these relevant CPDLC messages descriptors (each descriptor contains respective fields enclosed by single quotes for CPDLC Message where words surrounded with [] imply an INPUT that MUST be filled in by you if the message is chosen as translation, Intent and Reference Number) as context and choose the appropriate translation based on the intent of the input message:\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Translate this pilot/ATC message to CPDLC format and respond ONLY in JSON format where the response should ONLY contain ONE CPDLC translation as follows:\n",
        "        {{\n",
        "        \"reference\": [The correct reference number associated with the corresponding CPDLC message translation],\n",
        "        \"message\": [Translated CPDLC Message],\n",
        "        \"context\": [Additional information that the air traffic controller cannot convey through the CPDLC Message that should be given to the pilot. This field should be empty if no additional relevant information is present in the input based on intent.]\n",
        "        }}\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "tW7o69Sbg12S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from vllm import LLM, SamplingParams\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from openai import OpenAI\n",
        "\n",
        "# ----- Utilities -----\n",
        "class TranslationLogger:\n",
        "    def __init__(self):\n",
        "        self.__logs = []\n",
        "\n",
        "    def log(self, input, llm_response, is_success, rag_retrived_messages, expected_output) ->  None:\n",
        "        self.__logs.append((input, llm_response, is_success, rag_retrived_messages, expected_output))\n",
        "\n",
        "    def get_logs(self) -> list:\n",
        "        return self.__logs\n",
        "\n",
        "    def get_print_ready(self) -> str:\n",
        "        logs_str = \"\"\n",
        "        count = 1\n",
        "        for input, llm_response, is_success, rag_retrived_messages, expected_output in self.__logs:\n",
        "            logs_str += f\"------------- Log {count} -------------\"\n",
        "            logs_str += f\"\\nInput:                 {input}\"\n",
        "            logs_str += f\"\\nLLM Response:          {llm_response}\"\n",
        "            if expected_output != \"\":\n",
        "                logs_str += f\"\\nExpected Response:     {expected_output}\"\n",
        "            logs_str += f\"\\nValid Response:        {is_success}\"\n",
        "            logs_str += f\"\\nRetrieved Messages     {rag_retrived_messages}\"\n",
        "            count += 1\n",
        "\n",
        "        return logs_str\n",
        "\n",
        "\n",
        "# ----- Quantizations -----\n",
        "class QuantizedConfigs:\n",
        "    @staticmethod\n",
        "    def get_4bit_bnb() -> BitsAndBytesConfig:\n",
        "        return BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_8bit_bnb() -> BitsAndBytesConfig:\n",
        "        return BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# ----- Large Language Models Classes (OpenAI vs vLLM) -----\n",
        "class Model:\n",
        "    def __init__(self, model_name : str, model_config : dict, gen_params : dict = None):\n",
        "        self.model_name = model_name\n",
        "        self.model_config = model_config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.gen_params = gen_params\n",
        "        self._load_model(model_config)\n",
        "\n",
        "    def _load_model(self, model_config) -> None:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate(self, prompt, gen_params : dict = None) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def clear_memory(self) -> None:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_gen_params(self) -> dict:\n",
        "        return self.gen_params.copy()\n",
        "\n",
        "    def get_model_name(self) -> str:\n",
        "        return self.model_name\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_gen_params() -> dict:\n",
        "        return {\"temperature\" : 0.2, \"top_p\" : 0.9, \"max_tokens\" : 512}\n",
        "\n",
        "class VLLM_Model(Model):\n",
        "    def __init__(self, model_name : str, model_config : dict, gen_params : dict = None) -> None:\n",
        "        super().__init__(model_name, model_config)\n",
        "        self.gen_params = gen_params if gen_params is not None else Model.get_default_gen_params()\n",
        "        self.sampling_params = SamplingParams(**self.gen_params)\n",
        "\n",
        "    def _load_model(self, model_config) -> None:\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        os.environ['VLLM_USE_V1'] = '0'\n",
        "        self.model = LLM(model=self.model_name, **model_config, enforce_eager=True, disable_async_output_proc=True)\n",
        "\n",
        "    def generate(self, messages, gen_params : dict = None) -> str:\n",
        "        params = self.sampling_params if gen_params is None else SamplingParams(**gen_params)\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        outputs = self.model.generate([text], sampling_params=params, use_tqdm=False)\n",
        "        response = outputs[0].outputs[0].text\n",
        "        return response\n",
        "\n",
        "    def clear_memory(self) -> None:\n",
        "        del self.tokenizer\n",
        "        del self.model\n",
        "\n",
        "class OpenAI_Model(Model):\n",
        "    def __init__(self, model_name : str, model_config : dict, openai_api_key, openai_api_base : str = \"http://localhost:8000/v1\", gen_params : dict = None) -> None:\n",
        "        super().__init__(model_name, model_config)\n",
        "        self.gen_params = gen_params if gen_params is not None else Model.get_default_gen_params()\n",
        "        self.client = OpenAI(\n",
        "            api_key=openai_api_key,\n",
        "            base_url=openai_api_base,\n",
        "        )\n",
        "\n",
        "    def _load_model(self, model_config) -> None:\n",
        "        pass\n",
        "\n",
        "    def generate(self, messages, gen_params : dict = None) -> str:\n",
        "        params = self.gen_params if gen_params is None else gen_params\n",
        "        response = self.client.chat.completions.create(model=self.model_name, messages=messages, **params)\n",
        "        return response\n",
        "\n",
        "    def clear_memory(self) -> None:\n",
        "        self.client.close()\n",
        "\n",
        "# ----- CPDLC Retrieval Classes (RAG vs LLM) -----\n",
        "# Base Class of retrievals for which, if instantiated, all CPDLC messages are used in context\n",
        "class CPDLCRetriever:\n",
        "    WORD_TYPES = {\n",
        "        \"level\" : \"level\",\n",
        "        \"reach\" : \"level\",\n",
        "        \"above\" : \"level\",\n",
        "        \"below\" : \"level\",\n",
        "        \"between\" : \"level/time\",\n",
        "        \"climb\" : \"level/altitude\",\n",
        "        \"climbing\" : \"level/altitude\",\n",
        "        \"descent\" : \"level/altitude\",\n",
        "        \"descend\" : \"level/altitude\",\n",
        "        \"descending\" : \"level/altitude\",\n",
        "        \"maintain\" : \"level/altitude/speed\",\n",
        "        \"maintaining\" : \"level/altitude/speed\",\n",
        "        \"at\" : \"time/rate/level/speed\",\n",
        "        \"by\" : \"time\",\n",
        "        \"before\": \"time\",\n",
        "        \"after\" : \"time\",\n",
        "        \"less\" : \"speed\",\n",
        "        \"great\" : \"speed\",\n",
        "        \"offset\" : \"specified distance\",\n",
        "        \"contact\" : \"time\",\n",
        "        \"deviate\" : \"specified distance\",\n",
        "        \"deviation\" : \"specified distance\",\n",
        "        \"turn\" : \"degrees\",\n",
        "        \"heading\" : \"degrees\",\n",
        "        \"expect\" : \"speed/level\",\n",
        "        \"speed\" : \"speed\",\n",
        "        \"exceed\" : \"speed\",\n",
        "        \"contact\" : \"frequency\",\n",
        "        \"frequency\" : \"frequency\",\n",
        "        \"monitor\" : \"frequency\",\n",
        "        \"block\" : \"level\",\n",
        "        \"leaving\" : \"level\",\n",
        "        \"leave\" : \"level\",\n",
        "        \"accept\" : \"speed/level/distance\",\n",
        "        \"microphone\" : \"frequency\",\n",
        "        \"time\" : \"time\",\n",
        "        \"track\" : \"degrees\"\n",
        "    }\n",
        "\n",
        "    def __init__(self, cpdlc_data) -> None:\n",
        "        self.cpdlc_data = cpdlc_data\n",
        "        self.k = None\n",
        "\n",
        "    def retrieve_cpdlc(self, natural_language : str, k : int = 40) -> list:\n",
        "        parsed_message = self.__process_input_for_rag(natural_language)\n",
        "        return self._retrieve_top_k(parsed_message, k)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _retrieve_top_k(self, message : str, k : int) -> list:\n",
        "        return self.cpdlc_data\n",
        "\n",
        "    def __is_word_digit(self, word : str) -> bool:\n",
        "        if len(word) == 0:\n",
        "            return False\n",
        "        for c in word:\n",
        "            if not (c.isdigit() or c == '.' or c == \":\"):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def __get_numbers_indices(self, words : list[str]) -> list[int]:\n",
        "        indices = []\n",
        "        for idx, word in enumerate(words):\n",
        "            if self.__is_word_digit(word):\n",
        "                indices.append(idx)\n",
        "        return indices\n",
        "\n",
        "    def __process_input_for_rag(self, message : str) -> str:\n",
        "        words = message.split()\n",
        "        modified = False\n",
        "        if 'direct' in words:\n",
        "            direct_idx = words.index('direct')\n",
        "            words_len = len(words)\n",
        "            if direct_idx + 1 < words_len and words[direct_idx + 1] == 'to':\n",
        "                words = words[:direct_idx + 2] + ['position'] + (words[direct_idx + 2 : ] if direct_idx + 2 < words_len else [])\n",
        "            else:\n",
        "                words = words[:direct_idx + 1] + ['position'] + (words[direct_idx + 1 : ] if direct_idx + 1 < words_len else [])\n",
        "\n",
        "            modified = True\n",
        "\n",
        "        numbers = self.__get_numbers_indices(words)\n",
        "        if len(numbers) == 0 and not modified:\n",
        "            return message\n",
        "\n",
        "        # Make the changes\n",
        "        i = 0\n",
        "        last = None\n",
        "        for r in numbers:\n",
        "            while i < r:\n",
        "                if words[i] in self.WORD_TYPES:\n",
        "                    last = words[i]\n",
        "                i += 1\n",
        "            if ':' in words[r]:\n",
        "                words[r] = \"time\"\n",
        "            elif last is not None:\n",
        "                words[r] = self.WORD_TYPES[last]\n",
        "            i += 1\n",
        "        return \" \".join(words).strip()\n",
        "\n",
        "\n",
        "# CPDLC Retriver using RAG\n",
        "class RAG_CPDLCRetriver(CPDLCRetriever):\n",
        "    def __init__(self, cpdlc_data : list, embedder_name : str, k = None) -> None:\n",
        "        super().__init__(cpdlc_data)\n",
        "        self.embedder_name = embedder_name\n",
        "        self.embedder = SentenceTransformer(embedder_name)\n",
        "        self.__load_rag()\n",
        "        self.k = k\n",
        "\n",
        "    # Function to initialize the RAG mechanism with faix indexes, it uses the CPDLC message element as embeddings\n",
        "    def __load_rag(self) -> None:\n",
        "        # Create embeddings for all CPDLC messages\n",
        "        text_entries = [\n",
        "            item['Message_Element'].replace('[', '').replace(']', '').lower()\n",
        "            for item in self.cpdlc_data\n",
        "        ]\n",
        "        embeddings = self.embedder.encode(text_entries)\n",
        "\n",
        "        # Create FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    # Function to retrieve top corresponding cpdlc message elements\n",
        "    def _retrieve_top_k(self, message : str, k : int = 40) -> list:\n",
        "        query_embedding = self.embedder.encode([message])\n",
        "        _, indices = self.index.search(query_embedding, k)\n",
        "        return [self.cpdlc_data[i] for i in indices[0]]\n",
        "\n",
        "    def clear_memory(self):\n",
        "        del self.embedder\n",
        "        del self.index\n",
        "\n",
        "\n",
        "# CPDLC Retriver using a LLM (Still needs further developments)\n",
        "class LLM_CPDLCRetriver(CPDLCRetriever):\n",
        "    def __init__(self, cpdlc_data : list, model : Model, k = None) -> None:\n",
        "        super().__init__(cpdlc_data)\n",
        "        self.model = model\n",
        "        self.k = k\n",
        "\n",
        "    # Function to retrieve top corresponding cpdlc message elements\n",
        "    def _retrieve_top_k(self, message : str, k : int = 40) -> list:\n",
        "        response = self.model.generate(message)\n",
        "        return response.split(\"\\n\")[:k]\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.model.clear_memory()\n",
        "\n",
        "\n",
        "# ----- Main Translater Class -----\n",
        "class CPDLCTranslater:\n",
        "    def __init__(self, user : User, model : Model, retriever : CPDLCRetriever, additional_context : str = \"\") -> None:\n",
        "        self.user = user\n",
        "        self.model = model\n",
        "        self.retriever = retriever\n",
        "        self.additional_context = additional_context\n",
        "        self.message_history = {}\n",
        "        self.available_temps = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
        "\n",
        "    def change_model(self, model : Model, additional_context : str = None) -> None:\n",
        "        self.model = model\n",
        "        self.additional_context = additional_context\n",
        "\n",
        "    def change_retriever(self, retriever : CPDLCRetriever) -> None:\n",
        "        self.retriever = retriever\n",
        "\n",
        "    def add_recipient(self, recipient_id) -> None:\n",
        "        self.message_history[recipient_id] = []\n",
        "\n",
        "    def parse_llm_response(self, llm_response : str, prompt_end : str = None) -> tuple[str, bool]:\n",
        "        json_str = \"```json\"\n",
        "        if json_str in llm_response:\n",
        "            llm_response = llm_response[llm_response.index(json_str):]\n",
        "\n",
        "        if '{' not in llm_response or '}' not in llm_response:\n",
        "            return \"\", False\n",
        "\n",
        "        if '[' in llm_response or ']' in llm_response:\n",
        "            return llm_response, False\n",
        "\n",
        "        response = llm_response[llm_response.index('{'): llm_response.index('}') + 1]\n",
        "        return response, True\n",
        "\n",
        "    def translate(self, natural_language, logger : TranslationLogger = None, keep_history = False, recipient_id = None, k=40) -> dict:\n",
        "        # Retrieve top cpdlc corresponding messages as context\n",
        "        relevant_messages = self.retriever.retrieve_cpdlc(natural_language, k)\n",
        "\n",
        "        # Format the cpdlc descriptors\n",
        "        context_str = \"\\n\".join(\n",
        "            f\"CPDLC Message: '{msg['Message_Element']}'. Intent: '{msg['Message_Intent']}'. Reference Number: '{msg['Ref_Num']}'.\"\n",
        "            for msg in relevant_messages\n",
        "        )\n",
        "\n",
        "        # Format context and prepare input for llm\n",
        "        system_prompt = self.user.get_system_prompt()\n",
        "        prompt = system_prompt.format(context=context_str)\n",
        "        end_model_input = \"Input message:\\n\" + natural_language + self.additional_context + \"\\n\"\n",
        "\n",
        "        # Create the input for llm based on history or single message\n",
        "        message = {\"role\": \"user\", \"content\": prompt + end_model_input}\n",
        "        if keep_history and recipient_id is not None:\n",
        "            if recipient_id not in self.message_history:\n",
        "                self.message_history[recipient_id] = []\n",
        "            self.message_history[recipient_id].append(message)\n",
        "            messages = self.message_history[recipient_id]\n",
        "        else:\n",
        "            messages = [message]\n",
        "\n",
        "        # Loop while no valid response is given or count reaches 5\n",
        "        missing_valid_response = True\n",
        "        response_str = \"\"\n",
        "        count = 0\n",
        "        gen_params = None\n",
        "        while missing_valid_response:\n",
        "            # Get response\n",
        "            response_str_temp = self.model.generate(messages, gen_params)\n",
        "            response_str, success = self.parse_llm_response(response_str_temp, end_model_input)\n",
        "            missing_valid_response = not success\n",
        "\n",
        "            # Logging if needed\n",
        "            if logger is not None:\n",
        "                logger.log(natural_language, response_str, success, context_str, \"\")\n",
        "\n",
        "            # Handle case where the response is not valid\n",
        "            if missing_valid_response:\n",
        "                temperature = np.random.choice(self.available_temps)\n",
        "                gen_params = self.model.get_gen_params()\n",
        "                gen_params['temperature'] = temperature\n",
        "                count += 1\n",
        "                if count > 5:\n",
        "                    print(\"\\nLLM failed to generate valid response.\\n\")\n",
        "                    return {}\n",
        "\n",
        "        try:\n",
        "            response_json = json.loads(response_str)\n",
        "            return response_json\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"\\nFailed to parse json for llm response.\\n\")\n",
        "            return {}\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.model.clear_memory()\n",
        "        self.retriever.clear_memory()\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "3-7SLmrFmVN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d340c9ac-cdbd-4c56-c198-f1cac7d71973"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-15 15:53:17 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 05-15 15:53:17 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "import getpass\n",
        "\n",
        "# ----- Default Configurations -----\n",
        "DEFAULT_VLLM_MODEL = \"Qwen/Qwen2.5-14B-Instruct-AWQ\"\n",
        "DEFAULT_EMBEDDER = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "DEFAULT_TOP_K = 40\n",
        "DEFAULT_GEN_PARAMS = {\n",
        "    \"temperature\" : 0.2,\n",
        "    \"top_p\" : 0.9,\n",
        "    \"max_tokens\" : 512,\n",
        "    \"repetition_penalty\" : 1.00\n",
        "}\n",
        "\n",
        "# ----- Functions to validate the requirements -----\n",
        "PACKAGE_IMPORT_NAMES = {\"faiss-cpu\" : \"faiss\", \"sentence-transformers\" : \"sentence_transformers\"}\n",
        "\n",
        "def validate_reqs() -> tuple[bool, list[str]]:\n",
        "    \"\"\"Ensures all required packages are installed by checking requirements.txt\"\"\"\n",
        "    packages_missing = False\n",
        "    reqs_missing = []\n",
        "    with open('requirements.txt') as requirements:\n",
        "        for package in requirements:\n",
        "            package = package.replace(\"\\n\", \"\").strip()\n",
        "            package_found = importlib.util.find_spec(get_package_name(package)) is not None\n",
        "\n",
        "            if not package_found:\n",
        "                if not packages_missing:\n",
        "                    print(\"Packages missing:\")\n",
        "                    packages_missing = True\n",
        "\n",
        "                print(package)\n",
        "                reqs_missing.append(package)\n",
        "\n",
        "    return packages_missing, reqs_missing\n",
        "\n",
        "def get_package_name(package : str) -> str:\n",
        "    if package in PACKAGE_IMPORT_NAMES:\n",
        "        return PACKAGE_IMPORT_NAMES[package]\n",
        "\n",
        "    return package\n",
        "\n",
        "\n",
        "# ---- Helper Functions -----\n",
        "# Header function for consistency of UI\n",
        "def print_header(title):\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(f\"{title:^50}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# Gets a valid choice from the user\n",
        "def get_choice(prompt, options):\n",
        "    while True:\n",
        "        print(prompt)\n",
        "        for key, value in options.items():\n",
        "            print(f\"\\t{key}. {value}\")\n",
        "        choice = input(\"Enter your choice: \").strip()\n",
        "        if choice in options:\n",
        "            return choice\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# Gets input from the user with optional default, requirement, type, and validation.\n",
        "def get_input(prompt, default=None, required=False, input_type=str, validator=None):\n",
        "    while True:\n",
        "        display_prompt = f\"{prompt}\"\n",
        "        if default is not None:\n",
        "            display_prompt += f\" [{default}]\"\n",
        "        display_prompt += \": \"\n",
        "        value_str = input(display_prompt).strip()\n",
        "        if not value_str and default is not None:\n",
        "            value_str = str(default)\n",
        "\n",
        "        if not value_str and required:\n",
        "            print(\"This field is required.\")\n",
        "            continue\n",
        "\n",
        "        if not value_str and not required:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            value = input_type(value_str)\n",
        "            if validator and not validator(value):\n",
        "                continue\n",
        "            return value\n",
        "        except ValueError:\n",
        "            print(f\"Invalid input. Please enter a value of type {input_type.__name__}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "def get_yes_no(prompt, default_yes=True):\n",
        "    default_str = \"[Y/n]\" if default_yes else \"[y/N]\"\n",
        "    while True:\n",
        "        choice = input(f\"{prompt} {default_str}: \").strip().lower()\n",
        "        if not choice:\n",
        "            return default_yes\n",
        "        if choice in ['y', 'yes']:\n",
        "            return True\n",
        "        if choice in ['n', 'no']:\n",
        "            return False\n",
        "        print(\"Please enter 'y' or 'n'.\")\n",
        "\n",
        "# ----- Configuration Steps -----\n",
        "# Function to select the user role (Pilot or ATC)\n",
        "def select_user_role(app_config):\n",
        "    print_header(\"Step 1: Select User Role\")\n",
        "    options = {'1': 'Pilot (Translating Pilot Messages)', '2': 'ATC (Translating ATC Messages)'}\n",
        "    choice = get_choice(\"Choose the source of the messages to be translated:\", options)\n",
        "    if choice == '1':\n",
        "        app_config['role_class'] = Pilot\n",
        "        app_config['role_name'] = \"Pilot\"\n",
        "    else:\n",
        "        app_config['role_class'] = ATC\n",
        "        app_config['role_name'] = \"ATC\"\n",
        "    print(f\"User role set to: {app_config['role_name']}\")\n",
        "\n",
        "# Selects the LLM provider (vLLM or OpenAI)\n",
        "def select_model_provider(app_config, step=2):\n",
        "    print_header(f\"Step {step}: Select Model Provider\")\n",
        "    options = {'1': 'vLLM (Local Execution)', '2': 'OpenAI API (Server-based)'}\n",
        "    choice = get_choice(\"Choose how the LLM will be run:\", options)\n",
        "    app_config['model_provider'] = \"vLLM\" if choice == '1' else \"OpenAI\"\n",
        "    print(f\"Model provider set to: {app_config['model_provider']}\")\n",
        "\n",
        "# Configures the specific model and its parameters.\n",
        "def configure_model(app_config, step=3):\n",
        "    print_header(f\"Step {step}: Configure Language Model\")\n",
        "    provider = app_config['model_provider']\n",
        "    model_config = {}\n",
        "    gen_params_config = {}\n",
        "\n",
        "    # Model Name and Params Setup\n",
        "    if provider == \"vLLM\":\n",
        "        # Choose to use automatic/recommended settings or manual\n",
        "        options = {'1': 'Automatic (Recommended Model and Parameters)', '2': 'Manual Configuration'}\n",
        "        choice = get_choice(\"Choose how to setup the large language model:\", options)\n",
        "        automatic = choice == '1'\n",
        "\n",
        "        if automatic:\n",
        "            model_name = DEFAULT_VLLM_MODEL\n",
        "            model_config['quantization'] = None\n",
        "            model_config['tensor_parallel_size'] = 1\n",
        "        else:\n",
        "            model_name = get_input(\"Enter vLLM model name\", default=DEFAULT_VLLM_MODEL, required=True)\n",
        "            q_options = {'1': 'None', '2': 'AWQ (4-bit)', '3': 'BitsAndBytes (4-bit)'}\n",
        "            q_choice = get_choice(\"Choose Manual Quantization:\", q_options)\n",
        "            model_config['quantization'] = {'1': None, '2': 'awq', '3': 'bitsandbytes'}[q_choice]\n",
        "            model_config['tensor_parallel_size'] = get_input(\n",
        "                \"Enter tensor parallel size (e.g., 1)\", default=1, input_type=int,\n",
        "                validator=lambda x: x >= 1 or print(\"Must be >= 1\")\n",
        "            )\n",
        "    else:\n",
        "        model_name = get_input(\"Enter OpenAI model name\", default=\"gpt-4o\", required=True)\n",
        "        # Securely get API key\n",
        "        try:\n",
        "            api_key = getpass.getpass(prompt=\"Enter your OpenAI API Key: \")\n",
        "            if not api_key:\n",
        "                print(\"API Key is required for OpenAI.\")\n",
        "                sys.exit(1)\n",
        "            model_config['api_key'] = api_key\n",
        "        except Exception as e:\n",
        "            # getpass might fail in some environments\n",
        "            print(f\"\\nError getting API key: {e}\")\n",
        "            model_config['api_key'] = get_input(\"Enter your OpenAI API Key (visible input):\", required=True)\n",
        "\n",
        "    app_config['model_name'] = model_name\n",
        "    app_config['model_config'] = model_config\n",
        "\n",
        "    # Choose how to setup parameters\n",
        "    param_choice = get_choice(\n",
        "        \"Configure generation parameters:\",\n",
        "        {'1': 'Automatic (Use Defaults)', '2': 'Advanced (Customize)'}\n",
        "    )\n",
        "\n",
        "    if param_choice == '1':\n",
        "        gen_params_config = DEFAULT_GEN_PARAMS\n",
        "        print(\"Using default generation parameters.\")\n",
        "    else:\n",
        "        print(\"\\n--- Advanced Generation Parameter Configuration ---\")\n",
        "        current_params = DEFAULT_GEN_PARAMS.copy()\n",
        "        for key, default_value in DEFAULT_GEN_PARAMS.items():\n",
        "            input_type = int if isinstance(default_value, int) else float if isinstance(default_value, float) else str\n",
        "            validator = None\n",
        "            if key == 'temperature':\n",
        "                 validator=lambda x: 0.0 <= x <= 2.0 or print(\"Temperature must be between 0.0 and 2.0\")\n",
        "            elif key == 'top_p':\n",
        "                 validator=lambda x: 0.0 <= x <= 1.0 or print(\"Top_p must be between 0.0 and 1.0\")\n",
        "            elif key == 'max_tokens':\n",
        "                 validator=lambda x: x > 0 or print(\"Max_tokens must be positive\")\n",
        "            elif key == 'repetition_penalty':\n",
        "                 validator=lambda x: x > 0 or print(\"Repetition penalty must be positive\")\n",
        "\n",
        "            new_value = get_input(f\"Enter value for '{key}'\", default=default_value, input_type=input_type, validator=validator)\n",
        "            current_params[key] = new_value\n",
        "        gen_params_config = current_params\n",
        "        print(\"Advanced generation parameters set.\")\n",
        "\n",
        "    app_config['gen_params'] = gen_params_config\n",
        "\n",
        "# Configures the retrieval system (RAG or LLM).\n",
        "def configure_retrieval(app_config):\n",
        "    print_header(\"Step 4: Configure Retrieval System\")\n",
        "    retriever_config = {}\n",
        "\n",
        "    options = {'1': 'RAG (Recommended)', '2': 'Another LLM (NOT COMPLETED)', '3': 'None'}\n",
        "    choice = get_choice(\"Choose the retrieval system for CPDLC message context:\", options)\n",
        "\n",
        "    if choice == '1': # Rag\n",
        "        app_config['retriever_type'] = \"RAG\"\n",
        "        param_choice = get_choice(\n",
        "            \"Configure RAG parameters:\",\n",
        "            {'1': 'Automatic (Recommended)', '2': 'Advanced (Custom)'}\n",
        "        )\n",
        "        if param_choice == '1':\n",
        "            retriever_config['embedder_model'] = DEFAULT_EMBEDDER\n",
        "            retriever_config['k'] = DEFAULT_TOP_K\n",
        "            print(\"Using default RAG parameters.\")\n",
        "        else:\n",
        "            print(\"\\n--- Advanced RAG Parameter Configuration ---\")\n",
        "            retriever_config['embedder_model'] = get_input(\n",
        "                \"Enter Sentence Transformer embedder model name\",\n",
        "                default=DEFAULT_EMBEDDER, required=True\n",
        "            )\n",
        "            retriever_config['k'] = get_input(\n",
        "                \"Enter number of messages to retrieve (k)\",\n",
        "                default=DEFAULT_TOP_K, required=True, input_type=int,\n",
        "                validator=lambda x: x > 0 or print(\"k must be positive\")\n",
        "            )\n",
        "            print(\"Advanced RAG parameters set.\")\n",
        "    # TODO: This option does not work yet as prompts need to be added for LLM retrieval and safeguards\n",
        "    elif choice == '2':\n",
        "        app_config['retriever_type'] = \"LLM\"\n",
        "        print(\"Using the main LLM for retrieval tasks (configuration not needed here).\")\n",
        "    else:\n",
        "        app_config['retriever_type'] = None\n",
        "        print(\"No retrieval system will be used.\")\n",
        "\n",
        "    app_config['retriever_config'] = retriever_config\n",
        "\n",
        "# Initializes backend components based on configuration.\n",
        "def initialize_components(app_config) -> CPDLCTranslater:\n",
        "    print(\"\\n--- Initializing Components ---\")\n",
        "    try:\n",
        "        # 1. Initialize User Role\n",
        "        user_role = app_config['role_class']()\n",
        "        print(f\"Initialized User Role: {app_config['role_name']}\")\n",
        "\n",
        "        # 2. Initialize Model\n",
        "        model = None\n",
        "        if app_config['model_provider'] == \"vLLM\":\n",
        "            quantization_method = app_config['model_config']['quantization']\n",
        "            if quantization_method == 'awq':\n",
        "                if importlib.util.find_spec('autoawq') is None:\n",
        "                    quantization_method = 'bitsandbytes'\n",
        "                    print('Packakge autoawq not installed. Switched to bitsandbytes quantization')\n",
        "\n",
        "            # Actual vLLM initialization\n",
        "            try:\n",
        "                 num_gpus = 1\n",
        "                 if 'tensor_parallel_size' in app_config['model_config']:\n",
        "                      num_gpus = app_config['model_config']['tensor_parallel_size']\n",
        "\n",
        "                 # Pass model_config and gen_params in init\n",
        "                 model = VLLM_Model(\n",
        "                     model_name=app_config['model_name'],\n",
        "                     model_config={'quantization': quantization_method, 'tensor_parallel_size': num_gpus},\n",
        "                     gen_params=app_config['gen_params']\n",
        "                 )\n",
        "                 print(f\"\\nInitialized vLLM Model.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"\\n--- ERROR Initializing vLLM Model ---\")\n",
        "                 print(f\"Model: {app_config['model_name']}\")\n",
        "                 print(f\"Config used: {{'quantization': {quantization_method}, 'tensor_parallel_size': {num_gpus}}}\")\n",
        "                 print(f\"Error: {e}\")\n",
        "                 print(\"Check model name/path, available VRAM, and CUDA setup.\")\n",
        "                 print(\"\\nExiting Application.\")\n",
        "                 sys.exit(1)\n",
        "\n",
        "        elif app_config['model_provider'] == \"OpenAI\":\n",
        "            model = OpenAI_Model(\n",
        "                model_name=app_config['model_name'],\n",
        "                model_config=app_config['model_config'],\n",
        "                openai_api_key=app_config['model_config']['api_key'],\n",
        "                gen_params=app_config['gen_params']\n",
        "            )\n",
        "            print(f\"\\nInitialized OpenAI Model.\")\n",
        "\n",
        "        # 3. Initialize Retriever\n",
        "        retriever = None\n",
        "        if app_config['retriever_type'] == \"RAG\":\n",
        "            retriever = RAG_CPDLCRetriver(\n",
        "                cpdlc_data=user_role.get_cpdlc_data(),\n",
        "                embedder_name=app_config['retriever_config']['embedder_model'],\n",
        "                k=app_config['retriever_config']['k']\n",
        "            )\n",
        "            print(f\"\\nInitialized RAG Retriever.\")\n",
        "        elif app_config['retriever_type'] == \"LLM\":\n",
        "            # TODO: NOT DONE\n",
        "            raise NotImplementedError\n",
        "            retriever = LLM_CPDLCRetriver(\n",
        "                cpdlc_data=user_role.get_cpdlc_data(),\n",
        "                model=model)\n",
        "            print(\"\\nInitialized LLM Retriever (using main model)\")\n",
        "        else:\n",
        "            print(\"No retriever system selected.\")\n",
        "\n",
        "\n",
        "        # 4. Initialize Translator\n",
        "        translator = CPDLCTranslater(\n",
        "            user=user_role,\n",
        "            model=model,\n",
        "            retriever=retriever\n",
        "        )\n",
        "        print(\"\\nInitialized CPDLC Translator.\")\n",
        "        return translator\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- ERROR during component initialization ---\")\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please check your configuration and backend code.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def write_logs_to_file(logger : TranslationLogger):\n",
        "    logs_str = logger.get_print_ready()\n",
        "\n",
        "# ----- Main Application Logic -----\n",
        "def run_application():\n",
        "    count_conversations = 0\n",
        "    app_config = {}\n",
        "\n",
        "    # Configuration Steps\n",
        "    select_user_role(app_config)\n",
        "    select_model_provider(app_config)\n",
        "    configure_model(app_config)\n",
        "    configure_retrieval(app_config)\n",
        "\n",
        "    # Initialize Backend\n",
        "    translator = initialize_components(app_config)\n",
        "\n",
        "    # Interaction Loop\n",
        "    print_header(\"Step 5: Process Messages\")\n",
        "\n",
        "\n",
        "    # Ask if logs are to be kept\n",
        "    logger = None\n",
        "    keep_logs = get_yes_no(\"Do you want to keep all logs of execution?\", default_yes=False)\n",
        "    if keep_logs:\n",
        "        logger = TranslationLogger()\n",
        "\n",
        "    # Ask for conversation history\n",
        "    conversation_id = None\n",
        "    use_conversation = get_yes_no(\"Do you wish to use messages as part of a conversation (stores them for model messaging context history)?\", default_yes=False)\n",
        "    if use_conversation:\n",
        "        conversation_id = get_input(\"Enter a Conversation ID (or leave blank for automatic)\", required=False)\n",
        "        if not conversation_id:\n",
        "            count_conversations += 1\n",
        "            conversation_id = f\"CID_{count_conversations}\"\n",
        "        print(f\"Using Conversation ID: {conversation_id}\")\n",
        "        print(\"\\nTo change to a new conversation in program loop below, type 'change'.\")\n",
        "\n",
        "    store_history_flag = use_conversation\n",
        "    while True:\n",
        "        print(\"\\nEnter your natural language message below (or type 'exit' to quit)\")\n",
        "        natural_language_input = input(\"> \").strip()\n",
        "\n",
        "        if natural_language_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        if store_history_flag and natural_language_input.lower() == 'change':\n",
        "            conversation_id = get_input(\"Enter a Conversation ID (or leave blank for automatic)\", required=False)\n",
        "            if not conversation_id:\n",
        "                count_conversations += 1\n",
        "                conversation_id = f\"CID_{count_conversations}\"\n",
        "            print(f\"Using Conversation ID: {conversation_id}\")\n",
        "            continue\n",
        "\n",
        "        print(\"--- Processing ---\")\n",
        "        try:\n",
        "            response = translator.translate(\n",
        "                natural_language=natural_language_input,\n",
        "                keep_history=store_history_flag,\n",
        "                recipient_id=conversation_id,\n",
        "                logger=logger\n",
        "            )\n",
        "\n",
        "            print(\"\\n--- Translation Result ---\")\n",
        "            if response and 'message' in response and 'context' in response:\n",
        "                print(f\"- Instruction: {response['message']}\")\n",
        "                print(f\"- Context: {response['context']}\")\n",
        "                if 'reference' in response:\n",
        "                    print(f\"- Ref #: {response['reference']}\")\n",
        "            else:\n",
        "                print(\"Processing failed or returned empty result.\")\n",
        "            print(\"------------------------\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n--- ERROR during message processing ---\")\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    if logger is not None:\n",
        "        file_path = write_logs_to_file(logger)\n",
        "        print(f\"\\nWrote logs to file path: {file_path}\")\n",
        "\n",
        "    translator.clear_memory()\n",
        "    print(\"\\nExiting application. Goodbye!\")"
      ],
      "metadata": {
        "id": "CTDFgUdsnq-A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "a1pV8Vcvn_5R",
        "outputId": "e3d80fd0-78f0-456e-9652-a084231239c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ac9b92ca-26a9-478e-977d-30d657334a66\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ac9b92ca-26a9-478e-977d-30d657334a66\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving requirements.txt to requirements.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'requirements.txt': b'numpy\\npandas\\ntorch\\ntransformers\\naccelerate\\nbitsandbytes\\nsentence-transformers\\nfaiss-cpu\\nvllm'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate all packages are installed\n",
        "try:\n",
        "    packages_missing, _ = validate_reqs()\n",
        "    if packages_missing:\n",
        "        print(\"\\nPlease install the missing packages listed above to run this application.\\nExiting application.\")\n",
        "        sys.exit(1)\n",
        "except OSError or ValueError:\n",
        "    print(\"\\nUnexpected error occured while validating the requirements.\\nExiting application.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Run the application\n",
        "run_application()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dNvm-PGVs7Kk",
        "outputId": "a7895782-60ef-444d-fb37-3e7eab151ea4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "             Step 1: Select User Role             \n",
            "==================================================\n",
            "Choose the source of the messages to be translated:\n",
            "\t1. Pilot (Translating Pilot Messages)\n",
            "\t2. ATC (Translating ATC Messages)\n",
            "Enter your choice: 2\n",
            "User role set to: ATC\n",
            "\n",
            "==================================================\n",
            "          Step 2: Select Model Provider           \n",
            "==================================================\n",
            "Choose how the LLM will be run:\n",
            "\t1. vLLM (Local Execution)\n",
            "\t2. OpenAI API (Server-based)\n",
            "Enter your choice: 1\n",
            "Model provider set to: vLLM\n",
            "\n",
            "==================================================\n",
            "         Step 3: Configure Language Model         \n",
            "==================================================\n",
            "Choose how to setup the large language model:\n",
            "\t1. Automatic (Recommended Model and Parameters)\n",
            "\t2. Manual Configuration\n",
            "Enter your choice: 1\n",
            "Configure generation parameters:\n",
            "\t1. Automatic (Use Defaults)\n",
            "\t2. Advanced (Customize)\n",
            "Enter your choice: 1\n",
            "Using default generation parameters.\n",
            "\n",
            "==================================================\n",
            "        Step 4: Configure Retrieval System        \n",
            "==================================================\n",
            "Choose the retrieval system for CPDLC message context:\n",
            "\t1. RAG (Recommended)\n",
            "\t2. Another LLM (NOT COMPLETED)\n",
            "\t3. None\n",
            "Enter your choice: 1\n",
            "Configure RAG parameters:\n",
            "\t1. Automatic (Recommended)\n",
            "\t2. Advanced (Custom)\n",
            "Enter your choice: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using default RAG parameters.\n",
            "\n",
            "--- Initializing Components ---\n",
            "\n",
            "--- ERROR during component initialization ---\n",
            "Error: [Errno 2] No such file or directory: 'CPDLC_Uplink.json'\n",
            "Please check your configuration and backend code.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-44187c8fd628>\", line 245, in initialize_components\n",
            "    user_role = app_config['role_class']()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-c0bfb0d19488>\", line 64, in __init__\n",
            "    self._load_data(self.path)\n",
            "  File \"<ipython-input-2-c0bfb0d19488>\", line 22, in _load_data\n",
            "    with open(path, \"r\") as cpdlc_json:\n",
            "         ^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'CPDLC_Uplink.json'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-6-16d27c2f0d08>\", line 12, in <cell line: 0>\n",
            "    run_application()\n",
            "  File \"<ipython-input-4-44187c8fd628>\", line 338, in run_application\n",
            "    translator = initialize_components(app_config)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-4-44187c8fd628>\", line 321, in initialize_components\n",
            "    sys.exit(1)\n",
            "SystemExit: 1\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-44187c8fd628>\u001b[0m in \u001b[0;36minitialize_components\u001b[0;34m(app_config)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# 1. Initialize User Role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0muser_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'role_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initialized User Role: {app_config['role_name']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c0bfb0d19488>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cpdlc_data_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCPDLC_UPLINK_PATH\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcpdlc_data_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcpdlc_data_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c0bfb0d19488>\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcpdlc_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpdlc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpdlc_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CPDLC_Uplink.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-16d27c2f0d08>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Run the application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun_application\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-44187c8fd628>\u001b[0m in \u001b[0;36mrun_application\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m# Initialize Backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-44187c8fd628>\u001b[0m in \u001b[0;36minitialize_components\u001b[0;34m(app_config)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please check your configuration and backend code.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 1",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ]
}